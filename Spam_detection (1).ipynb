{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55c9b333-f404-4e92-941a-6b56faeddbb5",
   "metadata": {},
   "source": [
    "We use the requests library to send an HTTP GET request to the URL of the dataset. We check the status code of the response to determine if the download was successful (status_code == 200)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c7ebbfa-b142-4f6c-9956-9814f948f198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download successful\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "# URL of the dataset\n",
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "# Download the dataset\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    print(\"Download successful\")\n",
    "else:\n",
    "    print(\"Failed to download the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33597218-d295-45fc-b765-81197eb6f987",
   "metadata": {},
   "source": [
    "After downloading the dataset, we need to extract its contents. The dataset is provided in a .zip file format, which we will handle using Python's zipfile and io libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37cb4bdf-a865-408c-99a6-1286b826987f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction successful\n"
     ]
    }
   ],
   "source": [
    "# Extract the dataset\n",
    "with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "    z.extractall(\"sms_spam_collection\")\n",
    "    print(\"Extraction successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac6cd59-461a-4887-8b22-a84d43a362f3",
   "metadata": {},
   "source": [
    "The os.listdir function lists all files and directories in the specified path, allowing us to confirm that the SMSSpamCollection file is present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "545156df-5e1b-4209-a67a-fc28218ce229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted files: ['readme', 'SMSSpamCollection']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# List the extracted files\n",
    "extracted_files = os.listdir(\"sms_spam_collection\")\n",
    "print(\"Extracted files:\", extracted_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f74d1f6-3a92-439c-826a-18facd3fa8fb",
   "metadata": {},
   "source": [
    "Loading the Dataset\n",
    "With the dataset extracted, we can now load it into a pandas DataFrame for further analysis. The SMS Spam Collection dataset is stored in a tab-separated values (TSV) file format, which we specify using the sep parameter in pd.read_csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abae0af0-7539-4f7a-b384-2ade8a640e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\n",
    "    \"sms_spam_collection/SMSSpamCollection\",\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"label\", \"message\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19d8c95-6dec-429a-800c-7c97b931c482",
   "metadata": {},
   "source": [
    "After loading the dataset, it is important to inspect it for basic information, missing values, and duplicates. This helps ensure that the data is clean and ready for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a670370c-8bc0-43f0-a625-bd5554b38c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- HEAD --------------------\n",
      "  label                                            message\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
      "-------------------- DESCRIBE --------------------\n",
      "       label                 message\n",
      "count   5572                    5572\n",
      "unique     2                    5169\n",
      "top      ham  Sorry, I'll call later\n",
      "freq    4825                      30\n",
      "-------------------- INFO --------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   label    5572 non-null   object\n",
      " 1   message  5572 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Display basic information about the dataset\n",
    "print(\"-------------------- HEAD --------------------\")\n",
    "print(df.head())\n",
    "print(\"-------------------- DESCRIBE --------------------\")\n",
    "print(df.describe())\n",
    "print(\"-------------------- INFO --------------------\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27c89b82-35a1-4c77-b1de-4441bd8a848a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n",
      " label      0\n",
      "message    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values:\\n\", df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76aa089f-1038-48d3-906d-c167836f536e",
   "metadata": {},
   "source": [
    "Preprocessing spam dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "655ae583-77fb-4ba2-9bbd-e8e79e800f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BEFORE ANY PREPROCESSING ===\n",
      "  label                                            message\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Jihed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Jihed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jihed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the necessary NLTK data files\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "print(\"=== BEFORE ANY PREPROCESSING ===\") \n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce508eb6-d01b-4645-831c-9c51a610d0a6",
   "metadata": {},
   "source": [
    "Lowercasing the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0206495-4ffa-4f57-8326-a817a70548ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AFTER LOWERCASING ===\n",
      "0    go until jurong point, crazy.. available only ...\n",
      "1                        ok lar... joking wif u oni...\n",
      "2    free entry in 2 a wkly comp to win fa cup fina...\n",
      "3    u dun say so early hor... u c already then say...\n",
      "4    nah i don't think he goes to usf, he lives aro...\n",
      "Name: message, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Convert all message text to lowercase\n",
    "df[\"message\"] = df[\"message\"].str.lower()\n",
    "print(\"\\n=== AFTER LOWERCASING ===\")\n",
    "print(df[\"message\"].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4a1525-635d-4110-b63e-a245dfbf9f3a",
   "metadata": {},
   "source": [
    "Removing Punctuation and Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc9cb9b5-a444-4355-8413-e6637636bd50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AFTER REMOVING PUNCTUATION & NUMBERS (except $ and !) ===\n",
      "0    go until jurong point crazy available only in ...\n",
      "1                              ok lar joking wif u oni\n",
      "2    free entry in  a wkly comp to win fa cup final...\n",
      "3          u dun say so early hor u c already then say\n",
      "4    nah i dont think he goes to usf he lives aroun...\n",
      "Name: message, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Remove non-essential punctuation and numbers, keep useful symbols like $ and !\n",
    "df[\"message\"] = df[\"message\"].apply(lambda x: re.sub(r\"[^a-z\\s$!]\", \"\", x))\n",
    "print(\"\\n=== AFTER REMOVING PUNCTUATION & NUMBERS (except $ and !) ===\")\n",
    "print(df[\"message\"].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d65f6c6-e196-4f4d-a83f-72583831e012",
   "metadata": {},
   "source": [
    "Tokenizing the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d524d038-1e4e-4dbf-ac5b-f837bc312cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AFTER TOKENIZATION ===\n",
      "0    [go, until, jurong, point, crazy, available, o...\n",
      "1                       [ok, lar, joking, wif, u, oni]\n",
      "2    [free, entry, in, a, wkly, comp, to, win, fa, ...\n",
      "3    [u, dun, say, so, early, hor, u, c, already, t...\n",
      "4    [nah, i, dont, think, he, goes, to, usf, he, l...\n",
      "Name: message, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Split each message into individual tokens\n",
    "df[\"message\"] = df[\"message\"].apply(word_tokenize)\n",
    "print(\"\\n=== AFTER TOKENIZATION ===\")\n",
    "print(df[\"message\"].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4606516-8ada-4996-a4dd-75e064686ab9",
   "metadata": {},
   "source": [
    "Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5bd76de-5f4b-4c1e-88c5-016aa71781b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AFTER REMOVING STOP WORDS ===\n",
      "0    [go, jurong, point, crazy, available, bugis, n...\n",
      "1                       [ok, lar, joking, wif, u, oni]\n",
      "2    [free, entry, wkly, comp, win, fa, cup, final,...\n",
      "3        [u, dun, say, early, hor, u, c, already, say]\n",
      "4    [nah, dont, think, goes, usf, lives, around, t...\n",
      "Name: message, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Define a set of English stop words and remove them from the tokens\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "df[\"message\"] = df[\"message\"].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "print(\"\\n=== AFTER REMOVING STOP WORDS ===\")\n",
    "print(df[\"message\"].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2675d5e9-4771-48ae-a3e6-84c681ee57a7",
   "metadata": {},
   "source": [
    "Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4e03ef7-f68e-4f3e-b528-896ca6ef45dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AFTER STEMMING ===\n",
      "0    [go, jurong, point, crazi, avail, bugi, n, gre...\n",
      "1                         [ok, lar, joke, wif, u, oni]\n",
      "2    [free, entri, wkli, comp, win, fa, cup, final,...\n",
      "3        [u, dun, say, earli, hor, u, c, alreadi, say]\n",
      "4    [nah, dont, think, goe, usf, live, around, tho...\n",
      "Name: message, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Stem each token to reduce words to their base form\n",
    "stemmer = PorterStemmer()\n",
    "df[\"message\"] = df[\"message\"].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "print(\"\\n=== AFTER STEMMING ===\")\n",
    "print(df[\"message\"].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e554fae-96ab-438b-8995-59cf1ea62f67",
   "metadata": {},
   "source": [
    "Joining Tokens Back into a Single String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b271fba-fd0a-41d2-99e0-63145881b92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AFTER JOINING TOKENS BACK INTO STRINGS ===\n",
      "0    go jurong point crazi avail bugi n great world...\n",
      "1                                ok lar joke wif u oni\n",
      "2    free entri wkli comp win fa cup final tkt st m...\n",
      "3                  u dun say earli hor u c alreadi say\n",
      "4            nah dont think goe usf live around though\n",
      "Name: message, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Rejoin tokens into a single string for feature extraction\n",
    "df[\"message\"] = df[\"message\"].apply(lambda x: \" \".join(x))\n",
    "print(\"\\n=== AFTER JOINING TOKENS BACK INTO STRINGS ===\")\n",
    "print(df[\"message\"].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976f2b6f-f02c-4e97-a764-684b01bace31",
   "metadata": {},
   "source": [
    "Using CountVectorizer for the Bag-of-Words Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb38d4b-b643-4ae1-82d1-e0e8b988d8f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2736afce-3230-4ffc-ab3c-933359defebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize CountVectorizer with bigrams, min_df, and max_df to focus on relevant terms\n",
    "vectorizer = CountVectorizer(min_df=1, max_df=0.9, ngram_range=(1, 2))\n",
    "\n",
    "# Fit and transform the message column\n",
    "X = vectorizer.fit_transform(df[\"message\"])\n",
    "\n",
    "# Labels (target variable)\n",
    "y = df[\"label\"].apply(lambda x: 1 if x == \"spam\" else 0)  # Converting labels to 1 and 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e6a970-54ac-403a-9fc4-343d3d1f7635",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bd10bc02-fe29-443e-8ba8-7b750ac04d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Build the pipeline by combining vectorization and classification\n",
    "pipeline = Pipeline([\n",
    "    (\"vectorizer\", vectorizer),\n",
    "    (\"classifier\", MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3b3957-57e9-4900-b7e2-eedbc4e974ef",
   "metadata": {},
   "source": [
    "With the pipeline in place, we can easily integrate hyperparameter tuning to improve model performance. The objective is to find optimal parameter values for the classifier, ensuring that the model generalizes well and avoids overfitting.\n",
    "\n",
    "To achieve this, we use GridSearchCV. This method systematically searches through specified hyperparameter values to identify the configuration that produces the best performance. In the case of MultinomialNB, we focus on the alpha parameter, a smoothing factor that adjusts how the model handles unseen words and prevents probabilities from being zero. We can balance bias and variance by tuning alpha, ultimately improving the modelâ€™s robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a430aea-d7d8-4479-830d-0c342d2dadac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model parameters: {'classifier__alpha': 0.2}\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    \"classifier__alpha\": [0.01, 0.1, 0.15, 0.2, 0.25, 0.5, 0.75, 1.0]\n",
    "}\n",
    "\n",
    "# Perform the grid search with 5-fold cross-validation and the F1-score as metric\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"f1\"\n",
    ")\n",
    "\n",
    "# Fit the grid search on the full dataset\n",
    "grid_search.fit(df[\"message\"], y)\n",
    "\n",
    "# Extract the best model identified by the grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"Best model parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6868c6f-cef2-4403-b704-9a1bf6f53f55",
   "metadata": {},
   "source": [
    "Preprocessing New Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fd19cc80-157f-460b-b8ad-e5707db1ff11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Preprocess function that mirrors the training-time preprocessing\n",
    "def preprocess_message(message):\n",
    "    message = message.lower()\n",
    "    message = re.sub(r\"[^a-z\\s$!]\", \"\", message)\n",
    "    tokens = word_tokenize(message)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e879837e-05a2-4ecf-912f-a635922b63ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example SMS messages for evaluation\n",
    "new_messages = [\n",
    "    \"Congratulations! You've won a $1000 Walmart gift card. Go to http://bit.ly/1234 to claim now.\",\n",
    "    \"Hey, are we still meeting up for lunch today?\",\n",
    "    \"Urgent! Your account has been compromised. Verify your details here: www.fakebank.com/verify\",\n",
    "    \"Reminder: Your appointment is scheduled for tomorrow at 10am.\",\n",
    "    \"FREE entry in a weekly competition to win an iPad. Just text WIN to 80085 now!\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f31b1dbd-e3cc-425c-bec4-c8254dbc3a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess and vectorize messages\n",
    "processed_messages = [preprocess_message(msg) for msg in new_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0c2d81-e31a-4b9f-a7f8-1bd1dd045b21",
   "metadata": {},
   "source": [
    "Vectorizing the Processed Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9b6a030d-ac27-4b31-9315-7ae3470d4b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform preprocessed messages into feature vectors\n",
    "X_new = best_model.named_steps[\"vectorizer\"].transform(processed_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f2151392-96a8-46fb-b01c-051af990cab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with the trained classifier\n",
    "predictions = best_model.named_steps[\"classifier\"].predict(X_new)\n",
    "prediction_probabilities = best_model.named_steps[\"classifier\"].predict_proba(X_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7918c6f6-df3f-4537-a34f-1cc003fdf618",
   "metadata": {},
   "source": [
    "Displaying Predictions and Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "34b6db53-938b-4cb1-8d51-ec07dc02c063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: Congratulations! You've won a $1000 Walmart gift card. Go to http://bit.ly/1234 to claim now.\n",
      "Prediction: Spam\n",
      "Spam Probability: 1.00\n",
      "Not-Spam Probability: 0.00\n",
      "--------------------------------------------------\n",
      "Message: Hey, are we still meeting up for lunch today?\n",
      "Prediction: Not-Spam\n",
      "Spam Probability: 0.00\n",
      "Not-Spam Probability: 1.00\n",
      "--------------------------------------------------\n",
      "Message: Urgent! Your account has been compromised. Verify your details here: www.fakebank.com/verify\n",
      "Prediction: Spam\n",
      "Spam Probability: 0.96\n",
      "Not-Spam Probability: 0.04\n",
      "--------------------------------------------------\n",
      "Message: Reminder: Your appointment is scheduled for tomorrow at 10am.\n",
      "Prediction: Not-Spam\n",
      "Spam Probability: 0.00\n",
      "Not-Spam Probability: 1.00\n",
      "--------------------------------------------------\n",
      "Message: FREE entry in a weekly competition to win an iPad. Just text WIN to 80085 now!\n",
      "Prediction: Spam\n",
      "Spam Probability: 1.00\n",
      "Not-Spam Probability: 0.00\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Display predictions and probabilities for each evaluated message\n",
    "for i, msg in enumerate(new_messages):\n",
    "    prediction = \"Spam\" if predictions[i] == 1 else \"Not-Spam\"\n",
    "    spam_probability = prediction_probabilities[i][1]  # Probability of being spam\n",
    "    ham_probability = prediction_probabilities[i][0]   # Probability of being not spam\n",
    "    \n",
    "    print(f\"Message: {msg}\")\n",
    "    print(f\"Prediction: {prediction}\")\n",
    "    print(f\"Spam Probability: {spam_probability:.2f}\")\n",
    "    print(f\"Not-Spam Probability: {ham_probability:.2f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66308513-3ce9-41a1-94a6-78d784be53ce",
   "metadata": {},
   "source": [
    "Using joblib for Saving Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "61031cf3-be08-43ed-9f47-6fcabbb4ce4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to spam_detection_model.joblib\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained model to a file for future use\n",
    "model_filename = 'spam_detection_model.joblib'\n",
    "joblib.dump(best_model, model_filename)\n",
    "\n",
    "print(f\"Model saved to {model_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fdaf4bab-fb21-4c43-9181-a3cc1e1badda",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = joblib.load(model_filename)\n",
    "predictions = loaded_model.predict(new_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efcfb42-ab54-4e9d-8645-d393ad6bbc79",
   "metadata": {},
   "source": [
    "Model Evaluation (Spam Detection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93045372-1c46-4c6c-b4a5-29f88f5ec95f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
